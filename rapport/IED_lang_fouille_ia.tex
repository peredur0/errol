\documentclass[a4paper,12pt]{article}

% Module de base utf, français
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
% gestion des marges
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}

% quote
% \usepackage{csquotes}

% Packages math
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{mathrsfs}

% Package tableau
\usepackage{multirow}

% Package insertion d'image
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption} %image multiple

% Package url
\usepackage{url}	

% Package de couleur
\usepackage{color}
\usepackage[colorlinks,linkcolor=black,urlcolor=blue]{hyperref}

% Package pour le code, nécessite package color
\usepackage{listings}
\usepackage{amsmath}
\lstset{ % General setup for the package
	language=Python,
	basicstyle=\small\sffamily,
	numbers=left,
 	numberstyle=\tiny,
	frame=tb,
	tabsize=4,
	columns=fixed,
	showstringspaces=false,
	showtabs=false,
	keepspaces,
	commentstyle=\color{red},
	keywordstyle=\color{blue},
	breaklines=true
}

% Packages Verbatim
\usepackage{moreverb}

% nouvelle commande pour ~
%\newcommand{\tidle}{\char`\~}

\title{Projet L3\\Fouille de données\\Ingénierie des langues}
\author{GOEHRY Martial\\16711476}

\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	
	\section*{Introduction}
		\input{introduction}
		\newpage

	\section{Fouille de données}
		\label{sec:fouille-de-donnees}
		\input{fouille}
		\newpage
	
	\section{Ingénierie des langues}
		\label{sec:ingenierie-des-langues}
		\input{langues}
		\newpage

	\section{Modélisation}
		\label{sec:modelisation}
		\input{modelisation}
		\newpage

	\section*{Conclusion}
		\label{sec:Conclusion}
		Avec ce projet nous avons vu le montage d'une infrastructure basée sur docker pour le stockage des informations récoltées et générées lors des différentes phases.
		Les fonctions développées pour les bases SQL et NoSQL sont utilisées dans toutes les phases de traitement.\\

		Faute de sources, il n'a pas été possible de mettre en place une automatisation pour la récupération des mails.
		Il a été nécessaire de les récupérer manuellement dans d'autres projet du même type.\\

		Le traitement initiale des mails est totalement automatisé grâce à Python et aux modules email et re.
		Ce nettoyage a permis de ne conserver que l'essentiel des corps de message tout en écartant les mails corrompu, sans texte, ou avec un mauvais encodage.\\

		Pour agrémenter notre jeux de données la recherche de caractéristique s'est concentré sur la forme des textes ainsi que sur la présence d'éléments non textuel.
		Les résultats de cette recherche n'a pas été aussi concluant que ce qui était espéré.\\

		La phase de traitement du langage visait à réduire les textes au minimum en tentant de conserver un maximum de sens.
		Pour cela, la lemmatisation semblait la meilleure option.
		Le modèle neuronale développer par StandfordNLP à permis de réaliser cette opération. \\

		Enfin la vectorisation a été réalisée en utilisant la méthode TF-IDF (Term Frequency - Inverse Document Frequency).
		Cette méthode est en accord avec l'idée d'analyse les mails en se basant sur la présence de certains mots.
		L'ensemble des documents pu être généré mais des ajustements ont du être fait pour ne pas avoir de vecteur null.

		Le tableau~\ref{tab:evolution} montre l'évolution du nombre de document et des mots au fur et à mesure des phases du projet
		\begin{table}[H]
            \centering
            \begin{tabular}{l|rrr]}
                Phase/Etape & nombre de documents & nombre de mots & nombre de mots uniques\\
                \hline
				Récolte & 5798 & 2385120 & 262614\\
				Transformation & 5658 & 1222793 & 99772\\
				Sauvegarde & 5333 & 1163550 & 99772\\
				Traitement du langage & 5333 & 658524 & 39947\\
				Vectorisation & 5333 & 453068 & 2942\\
            \end{tabular}
            \caption{Problèmes possibles avec la mise en base}
            \label{tab:evolution}
        \end{table}

		L'utilisation d'une Random Tree Forest a permis d'entrainer un modèle avec un résultat de 98\% sur la classification des document.
		Cependant, il convient de valider ce résultat sur des messages n'ayant encore jamais été vu par Errol

	\newpage
	\appendix
	\section{Développement visualisation distribution de Zipf}
		\label{sec:devZipf}
		\input{dev-zipf}
	\newpage

	\section{Modèles}
		\label{sec:modeles}
		\input{modeles}
	\newpage

	\section{Bibliographie}
		\label{sec:bibliographie}
		\bibliographystyle{plain}
		\bibliography{IED_Lang_fouille}
	\newpage


	\section{Sitotec}
		\subsection{Corpus}
			\begin{itemize}
				\item Enron company mails, fichier CSV contenant l'ensemble des mails d'une entreprise ayant fermée ses portes (33.834.245 mails) [en ligne], \url{https://www.kaggle.com/wcukierski/enron-email-dataset} (consulté le 27/01/2022) \label{Enron_dataset}
				\item Mails project SpamAssassin, projet opensource de détection de spam (6065 fichiers email déjà trier en ham et spam) [en ligne], \url{https://spamassassin.apache.org/old/publiccorpus/} (consulté le 27/01/2022) \label{SpamAssassin_dataset}
				\item Brown corpus, ensemble de texte en anglais publié en 1961 qui contient plus d'un million de mots \url{https://www.nltk.org/book/ch02.html} (consulté le 20/08/2022) \label{Brown_corpus}
				\item Spam generated by LLM, fichier csv avec des mail généré par IA \url{https://www.kaggle.com/datasets/trainingdatapro/generated-e-mail-spam} (consulté le 06/08/2024) \label{spamLLM}
			\end{itemize}
		
		\subsection{Modules}
			\begin{itemize}
				\item Page Github du projet \emph{langdetect} capable de différencier 49 langages avec une précision de 99\%, [en ligne] \url{https://github.com/Mimino666/langdetect} (consulté le 04/12/2022) \label{langdetect}
				\item Language Detection Library, présentation du module (anglais) [en ligne] \url{https://www.slideshare.net/shuyo/language-detection-library-for-java} (consulté le 04/12/2022)
				\item Suite de cours et de ressources en ligne pour comprendre MongoDB et réussi a faire la connexion avec un programme Python; [en ligne] \url{https://learn.mongodb.com/learning-paths/mongodb-python-developer-path} (consulté le 09/2023)
				\item Documentation de la librairie standard python sqlite [en ligne] \url{https://docs.python.org/3/library/sqlite3.html} (consulté le 09/2023)
				\item Documentation de la librairie psycopg2 [en ligne] \url{https://pypi.org/project/psycopg2/} (consulté le 09/2023)
				\item Documentation de la librairie sqlalchemy [en ligne] \url{https://docs.sqlalchemy.org/en/20/index.html} (consulté le 09/2023)
			\end{itemize}
			
		\subsection{Modèles}
			\paragraph{Naïves Bayes}
				Le modèle Naïves Bayes est employé dans le module langdetect (\ref{langdetect})
				\begin{itemize}
					\item Les algorithmes de Naïves Bayes, Explication sommaire du principe de ces type d'algorithme, [en ligne] \url{https://brightcape.co/les-algorithmes-de-naives-bayes/} (consulté le 26/03/2023)
					\item Naive Bayes Classification Tutorial using Scikit-learn, exemple d'utilisation de ce type de modèle avec python (anglais) [en ligne] \url{https://www.datacamp.com/tutorial/naive-bayes-scikit-learn} (consulté le 26/03/2023)
					\item Scikit learn Naive Bayes, description des types d'algorithme disponibles dans le module Scikitlearn en python (anglais) [en ligne] \url{https://scikit-learn.org/stable/modules/naive_bayes.html} (consulté le 26/03/2023)
				\end{itemize}

			\paragraph{Random Tree Forest}
				Le modèle Random Tree Forest est utilisé dans la section~\ref{sec:modelisation}
				\begin{itemize}
					\item Formules mathématiques utilisées dans SciKit learn [en ligne] \url{https://scikit-learn.org/stable/modules/tree.html#tree-mathematical-formulation} (consulté le 06/08/2024)
				\end{itemize}

	\section{Code Source}
		\subsection{GitHub}
			L'ensemble du code est disponible dans mon repository GitHub. \url{https://github.com/peredur0/errol}
	
\end{document}





















